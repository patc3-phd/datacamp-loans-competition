---
title: "Placeholder"
output: html_notebook
---

# Optimizing Gower Weights in Feature Engineering (And Other Nearest-Neighbor Stories)
**By Patrick Coulombe, PhD (patrick.coulombe@d22consulting.com)**  
**Last updated `r format(Sys.time(), '%B %d, %Y')`**

## Nearest Neighbors as Features

In this publication we will:

* Compare nearest-neighbor methods used to engineer a new feature
* Optimize variable weights in a validation set for computing Gower distances
* 
* Create custom, re-usable functions and use pipelines instead of scripting, as is typical in R

The improvement with this dataset and this target (`not_fully_paid`: whether the loan is paid or not) is small, but I have applied this method in other contexts with great improvements in predictive performance. In particular, multiclass classification with a large number of classes seems to benefit from this approach.

**Don't forget to read the Key Takeaways section before you leave!**


## Description of the Method


## Load the Data

```{r}
v_target <- "not_fully_paid" # set target
source("loans_fn.R") # load custom functions to use in pipeline
df <- load_data(as_df = TRUE)
df <- df %>% slice_sample(n=3000) # to reduce computations in Workspace
str(df)
summary(df)
head(df)
```

## Prepare the Data

To prepare the data, we first make the `character` variable `purpose` into a `factor` for use as is when computing Gower distances, or for later dummy (one-hot) encoding for computing other distances (like Euclidian, Manhattan, etc.).

```{r}
# data prep
df <- cast(df, type_from="character", type_to="factor") # purpose is a factor
```

Next we split the dataset into a training and test set. We do this so that we can pick a nearest neighbor from the training set (which will be available in production at prediction time) and retrieve each neighbor's target value. This value will be added as a new column in the dataset and can be used as is to predict, or even better, be used as an additional feature in a predictive model (i.e., feature engineering).

Unexpectedly, we further split the training set into a training and a validation set. We will use this training set to find neighbors using different weights when computing Gower distances, and estimate the weights' performance on the validation set. We can then use these sets of optimized weights on our training set to derive neighbors on the test set. In other words, we treat the weights for each variable as hyperparameters to be tuned when computing Gower distances.


```{r}
# tt: train-test split
tt <- ttsplit(df, prop = .7) # list with tt$train and tt$test
# tv: train-validation split (to choose gower weights)
tv <- ttsplit(tt$train, prop = .7)
```

We then scale the numerical predictors in both the training and test sets using the mean and standard deviations *in the training set*, to avoid any target leakage (better safe than sorry!).

```{r}
# scale
tt <- scale_numeric_features_in_train_and_test(tt)
tv <- scale_numeric_features_in_train_and_test(tv)
```



## Optimizing Variable Weights in Gower Distances

### Gower Distance

A powerful yet lesser known method for computing distances between observations in the Gower distance. Originally published in 1971 and developed to compare text, the Gower distance between two observations is **the (weighted) average difference between the two observations on selected variables**.

For a categorical variable, the distance between two observations is 1 (the maximum) when the observations don't belong to the same category, and the distance is 0 (the minimum) when the observations belong to the same category.

For a numerical variable, the distance is scaled by taking account the range of possible values for that variable. For example, if Person #1 has a score of 5, Person #2 has a score of 7, and the min and max in the dataset are 1 and 7 (a Likert scale), then the distance is $abs(5-7)/(7-1) = 0.33$.

The final distance between two observations is the average distance across variables, weighted by the variables' weights. Typically the weights are equal (and they are by default), so that each variable contributes equally to the distance. However, one possibility is to optimize those weights to increase the predictive power of the nearest neighbor feature.

Note that with Gower, missing data are not a problem: They are simply ignored when computing the average distance across variables. For example, if the distance between two observations would normally be based on 10 variables but one observation has a variable missing, then the distance for this pair will be based on 9 variables. The dataset used in this publication does not have missing data, but **if your dataset has missing data, Gower is an ideal choice for computing distances**.

### Evaluating Random Combinations of Variables

To choose an optimal set of weights, we use an efficient, non-exhaustive (approximate) approach. For each run, we select variables at random to be used in the computation of distances (by setting their weight to 0 or 1 randomly). For each combination, we find the nearest neighbor (most similar observation) in the training set for each person in the validation set, and we assign that neighbor's target to a new feature in the validation set. This is in effect our best guess as to what the validation target is, based on the similarity of the training set.

Next we evaluate the performance of each combination of weights. For example, we could look at the match between actual target and the neighbor's target (i.e., accuracy). Given that the classes are imbalanced here, a better choice is either AUC or ROC curve (among others). Another good possibility is to look at the confusion matrices, to take into account both false positives and false negatives. 

Here we'll choose the best set of weights based on AUC. This is a good choice here, but it comes with an added difficulty: We need to transforming distances into approximate probabilities! Unlike confusion matrices, AUC (and ROC curves more generally) require soft classification (probabilities of belonging to the target category)rather than hard classification (a predicted category). I tried and compared several methods to achieve this here, and found that the best method was to normalize the distances and bring them to a 0-1 range:   
$prob(dist) = (dist - min(distances)) / (max(distances) - min(distances))$

I've also tried to normalize the log of distances, as well as the logistic function, and the logistic of the log distances. The logistic function applied to distances was the most natural one but it consistently yielded lower AUCs than simple normalization, while the logistic of log distances was equivalent to normalization.

```{r}
# loop to optimize weights
weights <- get_gower_weights(tv, min_vars = 3, n_combinations = 10) # generate possible combinations of 0-1 weights for each variable
metrics <- get_gower_metrics_for_weights(tv, weights_matrix = weights, eval_fn = yardstick::roc_auc) # compute AUC for each combination of weights
weights_max <- get_gower_best_weights(weights, metrics, choose_by=c("accuracy","mean_metric","auc")[3]) # extract best set of weights

# we could also manually choose our own weights (why not?)
weights_manual <- c(credit_policy=1, purpose=1, int_rate=0, installment=0, log_annual_inc=0, 
                    dti=0, fico=0, days_with_cr_line=0, revol_bal=0,
                    revol_util=0, inq_last_6mths=0, delinq_2yrs=0, pub_rec=1) %>% as.list %>% as.data.frame
```

## Comparing Methods for Calculating Distances and Finding Neighbors

Next we can compare different methods to calculate the distances used to find nearest neighbors in our dataset with our target variable. We do this by calculating the ROC curve for each method, and plotting the result. We include both versions of the Gower distance: the one with all variables equally weighted, and the one with the combination of variables that performed best on the validation set.

```{r}
#### compare nearest-neighbors methods with ROC curves ####

roc <- list()
roc$gower <- get_metrics_with_dist(tt, cluster::daisy, metric="gower", stand=TRUE, eval_fn = yardstick::roc_curve)
roc$gower_best <- get_metrics_with_dist(tt, cluster::daisy, metric="gower", stand=TRUE, weights=weights_max, eval_fn = yardstick::roc_curve)
roc$gower_manual <- get_metrics_with_dist(tt, cluster::daisy, metric="gower", stand=TRUE, weights=weights_manual, eval_fn = yardstick::roc_curve)

# make factor into dummy for numeric distances (e.g. euclidian for kNN)
if(!is.factor(tt$train[,v_target])) tt_num <- lapply(tt, make_factors_into_dummies) else tt_num <- tt
roc$euclidian <- get_metrics_with_dist(tt_num, fn=stats::dist, method="euclidian", eval_fn = yardstick::roc_curve)
roc$maximum <- get_metrics_with_dist(tt_num, fn=stats::dist, method="maximum", eval_fn = yardstick::roc_curve)
roc$manhattan <- get_metrics_with_dist(tt_num, fn=stats::dist, method="manhattan", eval_fn = yardstick::roc_curve)
roc$canberra <- get_metrics_with_dist(tt_num, fn=stats::dist, method="canberra", eval_fn = yardstick::roc_curve)
roc$binary <- get_metrics_with_dist(tt_num, fn=stats::dist, method="binary", eval_fn = yardstick::roc_curve)
roc$minkowski <- get_metrics_with_dist(tt_num, fn=stats::dist, method="minkowski", eval_fn = yardstick::roc_curve)

# plot
roc <- make_list_of_roc_curves_into_single_table(roc)
get_roc_curves_in_same_plot(roc) # comparing all methods
get_roc_curves_in_same_plot(roc %>% filter(model %in% c("gower", "gower_best"))) # comparing Gower methods

```


## Comparing a Random Forest With and Without Nearest Neighbor Features

Rather than simply using the Nearest Neighbor feature as a predictive model itself, we can add the Nearest Neighbor variable as an additional predictor in an actual predictive model. Here I illustrate with a random forest, which can often yield good results out-of-the-box with minimal tuning.

```{r}
#### Use nearest neighbor as feature(s) in random forest (i.e. feature engineering) ####
#### do ROC curve comparing w vs w/o added neighbor features

# no neighbor
(rf_no_neighbor <- get_rf_roc_curve(tt))

# with gower
tt_gower <- add_neighbor_target_from_dist_matrix(
                        tt, 
                        dist = get_dist(tt, cluster::daisy, metric="gower", stand=TRUE, weights=weights_max), 
                        p_add = TRUE)
(rf_gower <- get_rf_roc_curve(tt_gower))

# compare
get_roc_curves_from_random_forests(list(no_neighbor=rf_no_neighbor, gower=rf_gower))

```






## Key Takeaways




### Entire Pipeline ###




### Custom Functions (`loans_fn.R`)

```{r}
#### imports ####
library(dplyr)
library(cluster)
library(tidymodels)
library(vip)
req_pckgs <- c("fastDummies", "dataPreparation")
req_pckgs <- req_pckgs[which(!req_pckgs %in% rownames(installed.packages()))]
if(length(req_pckgs) != 0) install.packages(req_pckgs)

#### load data ####
load_data <- function(as_df=FALSE)
{
  df <- readr::read_csv("data/loans.csv.gz")
  if(as_df) class(df) <- "data.frame"
  return(df)
}

#### data prep ####
# make type_from variables into type_to
cast <- function(df, type_from, type_to)
{
  "
  Ex.: cast(df, 'numeric', 'factor') will make numeric variables in to factors
  
  "
  v <- sapply(df, \(x) inherits(x, type_from))
  v <- names(v)[which(v)]
  for(c in v) df[,c] <- do.call(paste0("as.", type_to), list(df[,c]))
  
  # out
  print(paste0("Made these variables from ", type_from, " to ", type_to, ":"))
  print(v)
  return(df)
}

# dummies
make_factors_into_dummies <- function(df, remove_first_dummy = TRUE, ignore_na = TRUE)
{
  df <- fastDummies::dummy_cols(df, remove_first_dummy = remove_first_dummy, ignore_na = ignore_na, remove_selected_columns = TRUE)
  
  # out
  print("Used fastDummies to make factors into dummies")
  return(df)
}


# normalize (range 0-1)
normalize <- function(x) (x-min(x, na.rm=TRUE))/(max(x, na.rm=TRUE)-min(x, na.rm=TRUE))


# logistic
logistic <- function(x) {1/(1+exp(-x))}



#### train test ####
# ttsplit
ttsplit <- function(df, prop_train=.7)
{
  df_split <- list()
  ix <- sample(nrow(df), round(nrow(df)*prop_train), replace=F)
  df_split$train <- df[ix,]
  df_split$test <- df[-ix,]
  print(paste0("Split df into list with: train, test (proportion train = ", prop_train, ")"))
  return(df_split)
}


# scale numeric vars
scale_numeric_features_in_train_and_test <- function(tt, exclude_target=TRUE)
{
  # scale numeric features train & test
  v_num <- colnames(tt$train)[which(sapply(tt$train, class) %in% c("numeric", "integer"))]
  if(exclude_target) v_num <- v_num[which(v_num != v_target)]
  scales <- dataPreparation::build_scales(data_set = tt$train, cols=v_num, verbose=TRUE)
  tt$train <- dataPreparation::fast_scale(data_set = tt$train, scales = scales, verbose = TRUE) %>% as.data.frame
  
  # test
  tt$test <- dataPreparation::fast_scale(data_set = tt$test, scales=scales, verbose=TRUE) %>% as.data.frame
  
  # out
  print("Created numerical scales using train set and rescaled numerical features in train and test")
  return(tt)
}



#### nearest neighbors ####

# generic fn to get distance matrix from selected fn
get_dist <- function(tt, fn, ...)
{
  # remove target
  tt_fv <- lapply(tt, \(df) {df[,v_target] <- NULL; return(df)})
  
  # get dist
  dist <- fn(do.call(rbind, tt_fv), ...)
  #dist <- daisy(do.call(rbind, tt_fv), metric="gower", stand=TRUE, weights=rep(1, ncol(tt_fv[[1]])))
  
  # out
  return(dist)
}


add_neighbor_target_from_dist_matrix <- function(tt, dist, p_add=TRUE, p_fn=normalize)
{
  "
  input:  tt is train-test list
          dist is distance or dissimilarity matrix
          p_add is whether to add probability (for ROC)
          p_fn is function to convert dist to probabilities (needs to be monotonic)
  output: df with neighbor added
  "
  # create the dist matrix
  dist <- as.matrix(dist)
  
  # remove (i,i) entries to remove self selection
  diag(dist) <- NA
  
  # transform to probabilities (no attempt to calibrate)
  if(p_add) dist <- p_fn(dist) # could take log as well; also logistic() with or without log
  
  # limit choice to train obs
  n_train <- nrow(tt$train)
  n_test <- nrow(tt$test) 
  dist <- dist[,1:n_train] # remove test columns
  
  # get index in train set
  ix <- apply(dist, 1, which.min)
  ix_train <- ix[1:n_train]
  ix_test <- ix[(n_train+1):length(ix)]
  
  # add target from train
  col <- "nn"#paste0("nn_", round(runif(n=1, min=9999, max=99999999), 0))
  tt$train[,col] <- tt$train[ix_train,v_target]
  tt$test[,col] <- tt$train[ix_test,v_target]
  
  # add p
  if(p_add)
  {
    p_col <- "nn_p"
    #set p_yes to similarity (1-diss) if neighbor target is 1, or diss if neighbor is 0
    min_dists <- sapply(seq_along(rownames(dist)), \(i) dist[i, ix[i]])
    ix_yes <- which(do.call(rbind, tt)[,col] %in% c(1, "1", max(tt$train[,v_target]))) # max() is because target is standardized
    min_dists[ix_yes] <- 1-min_dists[ix_yes]
    # add
    tt$train[,p_col] <- min_dists[1:n_train]
    tt$test[,p_col] <- min_dists[(n_train+1):length(min_dists)]
  }
  
  # out
  print(paste("Added column(s) to train and test:", col, if(p_add) p_col else NULL))
  return(tt)
}


# get metrics
get_metrics <- function(tt, eval_fn=yardstick::conf_mat, ...)
{
  "
  ... to be passed to conf_mat
  "
  
  # eval fn & estimate var
  if(is.null(eval_fn)) eval_fn <- yardstick::conf_mat
  if(isTRUE(all.equal(eval_fn, yardstick::conf_mat))) nn_var <- "nn" else nn_var <- "nn_p"
  
  
  # make target and nn factors (conf_mat only takes factors)
  tt <- lapply(tt, cast, "numeric", "factor")
  
  # top level comes first (for AUC)
  tt <- lapply(tt, \(df) { df[,v_target] <- factor(df[,v_target], levels=sort(levels(df[,v_target]), decreasing = TRUE)) ; df } )

  # make sure nn and v_target have the same factor levels
  if(nn_var=="nn") tt <- lapply(tt, \(df){ df[,nn_var] <- factor(df[,nn_var], levels=levels(tt$train[,v_target])); df })
  
    # nn_p is numeric
  if(nn_var=="nn_p") tt <- lapply(tt, \(df) { df[,nn_var] <- as.numeric(as.character(df[,nn_var])) ; df } ) # sloppy: goes from num to factor to num
 
  # get metrics
  metrics <- list(
    train=eval_fn(tt$train, truth=!!v_target, estimate=!!nn_var),
    test=eval_fn(tt$test, truth=!!v_target, estimate=!!nn_var)
  )
  
  # print summary
  print(lapply(metrics, summary))
  
  # out
  return(metrics)
}


# pipeline to get metrics from tt and dist function
get_metrics_with_dist <- function(tt, fn=NULL, ..., eval_fn=NULL)
{
  dist <- get_dist(tt, fn=fn, ...)
  tt <- add_neighbor_target_from_dist_matrix(tt, dist)
  
  # metrics
  get_metrics(tt, eval_fn)
  
}


# get gower weights
get_gower_weights <- function(tt, min_vars=1, n_combinations=NULL)
{
  
  # arg check
  if(min_vars<1) stop("min_var needs to be >= 1")
  
  # define weight matrix
  weights <- list()
  for (i in 1:(ncol(tt$train)-1)) weights[[i]] <- c(0,1)
  weights <- expand.grid(weights)
  colnames(weights) <- (.n <- colnames(tt$train))[.n != v_target]
  
  # filtering
  weights <- weights[rowSums(weights) >= min_vars,] # remove row where all weights are 0
  if(!is.null(n_combinations)) weights <- weights %>% slice_sample(n=min(nrow(.), n_combinations))
  
  # out
  return(weights)
}


# optimize weights in gower
get_gower_metrics_for_weights <- function(tt, weights_matrix=NULL, eval_fn=yardstick::conf_mat)
{
  # define weight matrix
  weights <- if(is.null(weights_matrix)) get_gower_weights(tt) else weights_matrix
  
  # get metrics for all weights
  metrics <- list()
  for (i in 1:nrow(weights)) 
  {
    sink("NUL")
    m <- get_metrics_with_dist(tt, fn=cluster::daisy, metric="gower", stand=TRUE, weights=weights[i,], eval_fn=eval_fn)
    metrics[[i]] <- if(isTRUE(all.equal(eval_fn, yardstick::conf_mat))) lapply(m, summary) else m
    sink()
  }
  
  # out
  return(metrics)
}


# retrieve best weights for gower
get_gower_best_weights <- function(weights, metrics, choose_by=c("accuracy", "mean_metric", "auc"))
{
  "
  input:  weights used to get metrics list (from get_gower_weights())
          metrics is metrics list (from get_gower_metrics_for_weights())
  output: matrix with 1 row (best weights)
  "
  if (choose_by == "accuracy")
  {
    # avg train and test because all high values in test seem more normal in train
    acc_tt <- sapply(metrics, \(m) mean(sapply(m, \(tbl) tbl %>% filter(.metric=="accuracy") %>% pull(.estimate))))
    acc_tt[which(sapply(metrics, \(m) any(sapply(m, \(tbl) any(is.na(tbl$.estimate))))))] <- NA # remove only one prediction
  }
  
  if (choose_by == "mean_metric")
  {
    acc_tt <- sapply(metrics, \(m)sapply(m, \(tbl) mean(tbl$.estimate)))
    acc_tt <- colMeans(acc_tt)
    acc_tt[which(sapply(metrics, \(m) any(sapply(m, \(tbl) any(tbl$.estimate < 0)))))] <- NA # remove any weights combination that yield negative metrics estimates
  }
  
  if (choose_by == "auc")
  {
    acc_tt <- sapply(metrics, \(m)sapply(m, \(tbl) tbl %>% pull(.estimate)))
    acc_tt <- colMeans(acc_tt)
  }
  
  # need to compare to actual test set when using a set to choose weights
  # ie should do tv (train valid) then tt
  # choose weights and run with tt to get test metrics
  weights_max <- weights[which.max(acc_tt),]
  
  # out
  return(weights_max)
}



# several roc curves in same graph
# need the table roc curve table
# x=1-specificity, y=sensitivity, color=model
# https://sydykova.com/post/2019-03-12-make-roc-curves-tidyverse/

make_list_of_roc_curves_into_single_table <- function(roc)
{
  "
  input:  roc list (one per distance)
  output: single table of roc curves with added model column
  "
  roc <- lapply(roc, \(l) l$test)  
  roc <- lapply(seq_along(roc), \(i) { roc[[i]]$model <- names(roc)[i]; roc[[i]] } ) %>% bind_rows()
  return(roc)
}

get_roc_curves_in_same_plot <- function(roc_tbl)
{
  "
  input:  roc_tbl is single table with yardstick::roc_curve and bind_rows() with added model column
  output: prints plot
  "
  roc_tbl %>% 
    ggplot(aes(x=1-specificity, y=sensitivity, color=model)) +
    geom_line(size=1.5) +
    geom_abline(slope = 1, intercept = 0, size = 0.4, linetype="dashed") +
    coord_fixed() + # fixed aspect ratio
    theme_gray(base_size=24)
}


# get roc curve and variable importance from random forest
get_rf_roc_curve <- function(tt)
{
  "
  input:  tt is train-test list
  output: list with variable importance ($var_imp) and roc curve ($roc)
  "
  
  # make levels TRUE or FALSE
  tt <- lapply(tt, \(df) { df[,v_target] <- factor(df[,v_target], levels=sort(unique(df[,v_target]), decreasing = TRUE)) ; levels(df[,v_target]) <- c("TRUE", "FALSE") ; df } )
  
  # Random Forest with var imp. and ROC curve
  rand_forest() %>% 
    set_mode("classification") %>%
    set_engine("randomForest") %>% 
    fit(factor(not_fully_paid) ~ ., tt$train) %T>%
    { vip(.) ->> var_imp } %>% # save variable importance
    predict(tt$test, type="prob") %>% 
    bind_cols(tt$test) %T>% 
    { roc_curve(., truth=not_fully_paid, estimate=.pred_TRUE) ->> roc } %>% # save ROC curve
    roc_auc(truth=not_fully_paid, estimate=.pred_TRUE) -> auc # save AUC
  
  # out
  return(list(var_imp=var_imp, roc=roc, auc=auc))
}

# get plots for several random forests
get_roc_curves_from_random_forests <- function(rf_list)
{
  rf_list <- lapply(seq_along(rf_list), \(i) { rf_list[[i]]$roc$model <- names(rf_list)[i]; rf_list[[i]]$roc } )
  rf_list %>% 
    bind_rows() %>% 
    get_roc_curves_in_same_plot()
}

```



